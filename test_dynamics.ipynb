{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4452, grad_fn=<MeanBackward1>)\n",
      "tensor(0.1108, grad_fn=<MeanBackward1>)\n",
      "tensor(0.0865, grad_fn=<MeanBackward1>)\n",
      "tensor(0.0868, grad_fn=<MeanBackward1>)\n",
      "tensor(0.0661, grad_fn=<MeanBackward1>)\n",
      "tensor(0.0885, grad_fn=<MeanBackward1>)\n",
      "tensor(0.0507, grad_fn=<MeanBackward1>)\n",
      "tensor(0.0778, grad_fn=<MeanBackward1>)\n",
      "tensor(0.0903, grad_fn=<MeanBackward1>)\n",
      "tensor(0.0467, grad_fn=<MeanBackward1>)\n",
      "tensor(0.0767, grad_fn=<MeanBackward1>)\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "tensor([[-0.0656],\n",
      "        [ 0.9974],\n",
      "        [ 0.9974],\n",
      "        [ 0.9974],\n",
      "        [ 0.5229],\n",
      "        [ 0.9974],\n",
      "        [ 0.9974],\n",
      "        [ 0.0072],\n",
      "        [ 0.9974],\n",
      "        [-0.0139]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(4, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "#         self.fc3 = nn.Linear(100, 100)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import numpy as np \n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    model = Encoder().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    model.train()\n",
    "    for itr in range(10000):\n",
    "        x = np.random.uniform(size=(100,2))\n",
    "        y = np.random.uniform(size=(100,2))\n",
    "        targ = (np.linalg.norm(x-y, axis=1) <= 0.4).reshape(-1, 1).astype(np.float32)\n",
    "        attn = model(torch.Tensor(np.concatenate((x, y), axis=1), device=device))\n",
    "#         attn = model(torch.Tensor(np.linalg.norm(x-y, axis=1).reshape(-1, 1), device=device))\n",
    "#         e_x = model(torch.Tensor(x, device=device))\n",
    "#         e_y = model(torch.Tensor(y, device=device))\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.mean(\n",
    "            torch.abs(attn - torch.Tensor(targ, device=device)),\n",
    "        )\n",
    "        if itr % 1000 == 0:\n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n",
    "    x = np.random.uniform(size=(10,2))\n",
    "    y = np.random.uniform(size=(10,2))\n",
    "    targ = (np.linalg.norm(x-y, axis=1) <= 0.4).reshape(-1, 1).astype(np.float32)\n",
    "    attn = model(torch.Tensor(np.concatenate((x, y), axis=1), device=device))\n",
    "    print(targ)\n",
    "    print(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvin/anaconda3/envs/rl/lib/python3.5/site-packages/ipykernel/__main__.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0092, grad_fn=<MeanBackward1>)\n",
      "tensor(0.0056, grad_fn=<MeanBackward1>)\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.8579, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.1421, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.8287, 0.1713, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0154, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.9846, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 1.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "       grad_fn=<AsStridedBackward>)\n",
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvin/anaconda3/envs/rl/lib/python3.5/site-packages/ipykernel/__main__.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes=[2, 20, 20, 1]):\n",
    "        # num_obj_classes includes background class\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            self.add_module('layer_{}'.format(i), self.layers[i])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = F.relu(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "    \n",
    "model = MLP([2, 100, 25])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "model.train()\n",
    "for itr in range(1000):\n",
    "    x = np.random.uniform(-1, 1, size=(100, 2))\n",
    "    inds = (2*(x+1)).astype(np.uint8)\n",
    "    targ = np.zeros((100, 5, 5), dtype=np.float32)\n",
    "    for i in range(100):\n",
    "        targ[i,inds[i][0],inds[i][1]] = 1.\n",
    "    out = torch.nn.Softmax()(model(torch.Tensor(x)))\n",
    "    out = torch.reshape(out, (-1, 5, 5))\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.mean(\n",
    "        torch.abs(out - torch.Tensor(targ)),\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss)\n",
    "# tests = []\n",
    "# for x in range(-2, 3):\n",
    "#     for y in range(-2, 3):\n",
    "#         tests.append([x/2.,y/2.])\n",
    "# tests = np.array(tests)\n",
    "# print((2*(tests+1)).astype(np.uint8))\n",
    "x = np.random.uniform(-1, 1, size=(100, 2))\n",
    "inds = (2*(x+1)).astype(np.uint8)\n",
    "targ = np.zeros((100, 5, 5), dtype=np.float32)\n",
    "for i in range(100):\n",
    "    targ[i,inds[i][0],inds[i][1]] = 1.\n",
    "out = torch.nn.Softmax()(model(torch.Tensor(x)))\n",
    "out = torch.reshape(out, (-1, 5, 5))\n",
    "loss = torch.mean(\n",
    "    torch.abs(out - torch.Tensor(targ)),\n",
    ")\n",
    "print(loss)\n",
    "print(out)\n",
    "print(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "data = h5py.File('data/obj_balls.h5', 'r')\n",
    "\n",
    "ims = np.zeros((5, 3, 64, 64), dtype=np.float32)\n",
    "\n",
    "locs = np.where(data['training']['groups'][:1, :5][0] == 2)\n",
    "ims[locs[0], np.zeros_like(locs[3]), locs[1], locs[2]] = 1.\n",
    "\n",
    "locs = np.where(data['training']['groups'][:1, :5][0] == 1)\n",
    "ims[locs[0], 2*np.ones_like(locs[3]), locs[1], locs[2]] = 1.\n",
    "\n",
    "import cv2\n",
    "cv2.imshow('im', (255*ims[0]).astype(np.uint8).transpose([1,2,0]))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9666, grad_fn=<MeanBackward1>) tensor([-0.6943,  0.8889,  0.9955, -0.4030,  1.3577,  1.0361, -0.2839, -0.1639,\n",
      "         0.7220,  0.3152], grad_fn=<SelectBackward>) [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "tensor(0.0566, grad_fn=<MeanBackward1>) tensor([ 0.0053,  0.0049, -0.0154,  0.2577,  0.3687,  0.2930,  0.1761,  0.0571,\n",
      "        -0.0068, -0.1126], grad_fn=<SelectBackward>) [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1001) must match the size of tensor b (1000) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d0bd39425e24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     loss = torch.mean(\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mreconstr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     69\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1001) must match the size of tensor b (1000) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes=[1, 100, 100, 100, 2]):\n",
    "        # num_obj_classes includes background class\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            self.add_module('layer_{}'.format(i), self.layers[i])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = F.elu(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "    \n",
    "model = MLP([1, 100, 10])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "model.train()\n",
    "for itr in range(1000):\n",
    "    inds = np.random.randint(0, 8, size=1000, dtype=np.uint8)\n",
    "    x = np.zeros((1000, 10))\n",
    "    y = np.zeros((1000, 10))\n",
    "    x[np.arange(1000),inds] = 1\n",
    "    x += np.random.normal(0, 0.1, size=(1000, 10))\n",
    "    y[np.arange(1000),inds+2] = 1\n",
    "    \n",
    "    x_var = Variable(torch.Tensor(x), requires_grad=True)\n",
    "#     reconstr = model(x_var)\n",
    "    reconstr = torch.zeros((1000, 10))\n",
    "    inds = (x_var >= 0.5).nonzero()\n",
    "    reconstr = model(inds[:,1].type(torch.FloatTensor).reshape(-1, 1)) #for objects: reshape, and sum, and reshape again\n",
    "#     shifts = Variable(model(torch.Tensor(np.expand_dims(inds, axis=1))), requires_grad=True)\n",
    "# #     shifts = np.ones((1000, 2))*0.001\n",
    "# #     shifts[:,0] = (inds + 2 - 4.5)/4.5\n",
    "# #     shifts = torch.Tensor(shifts)\n",
    "    \n",
    "# #     m = torch.distributions.Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "# #     samples = m.sample(torch.Size([100]))[:,0]\n",
    "# #     samples *= shifts[:,1]\n",
    "# #     samples += shifts[:,0]\n",
    "# #     samples = torch.clamp(samples, -4.5, 4.498)\n",
    "# #     samples += 4.5\n",
    "# #     reconstr = torch.zeros((100, 10))\n",
    "# #     for i in range(100):\n",
    "# #         reconstr[i].index_fill_(0, torch.floor(samples[i]).type(torch.LongTensor), samples[i]-torch.floor(samples[i]))\n",
    "# #         reconstr[i].index_fill_(0, torch.ceil(samples[i]+0.001).type(torch.LongTensor), torch.ceil(samples[i]+0.001)-samples[i])\n",
    "# #     print(reconstr[0], y[0])\n",
    "#     reconstr = torch.zeros((1000, 10))\n",
    "#     softmax = torch.nn.Softmax()\n",
    "#     for i in range(1000):\n",
    "#         row = torch.zeros(10)\n",
    "#         for j in range(10):\n",
    "#             row[j] = -((j-4.5)/4.5-shifts[i,0])**2 #/ shifts[i,1]\n",
    "#         reconstr[i] = softmax(row)\n",
    "# #         print(reconstr[i])\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.mean(\n",
    "        (reconstr - torch.Tensor(y))**2,\n",
    "    )\n",
    "    loss.backward()\n",
    "#     print(shifts.grad[0, 0])\n",
    "#     print(x_var.grad)\n",
    "    optimizer.step()\n",
    "#     print(loss, (inds[0]+2-4.5)/4.5, shifts[0, 0])\n",
    "    if itr % 100 == 0:\n",
    "        print(loss, reconstr[0], y[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward time 0.3080730438232422\n",
      "backward time 0.6057374477386475\n",
      "tensor(7.4053, grad_fn=<MeanBackward1>) tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, 21.9304, 16.2698, -9.9220, 27.0069, -3.5996,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000], grad_fn=<SelectBackward>) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "forward time 0.3587963581085205\n",
      "backward time 0.603090763092041\n",
      "forward time 0.384920597076416\n",
      "backward time 0.7779502868652344\n",
      "forward time 0.3351554870605469\n",
      "backward time 1.291853427886963\n",
      "forward time 0.4275484085083008\n",
      "backward time 0.8399453163146973\n",
      "forward time 0.47202277183532715\n",
      "backward time 0.8670227527618408\n",
      "forward time 0.36507129669189453\n",
      "backward time 0.789811372756958\n",
      "forward time 0.4984769821166992\n",
      "backward time 0.6993107795715332\n",
      "forward time 0.49492835998535156\n",
      "backward time 0.7373406887054443\n",
      "forward time 0.3706371784210205\n",
      "backward time 0.6911165714263916\n",
      "forward time 0.3968088626861572\n",
      "backward time 0.6934270858764648\n",
      "forward time 0.4930226802825928\n",
      "backward time 0.6432652473449707\n",
      "forward time 0.3299727439880371\n",
      "backward time 0.5936667919158936\n",
      "forward time 0.3853633403778076\n",
      "backward time 0.6109843254089355\n",
      "forward time 0.35195493698120117\n",
      "backward time 0.6622617244720459\n",
      "forward time 0.3216578960418701\n",
      "backward time 0.6096491813659668\n",
      "forward time 0.33248114585876465\n",
      "backward time 0.6038591861724854\n",
      "forward time 0.32401227951049805\n",
      "backward time 0.602900505065918\n",
      "forward time 0.31375980377197266\n",
      "backward time 0.6450850963592529\n",
      "forward time 0.3733329772949219\n",
      "backward time 0.7453391551971436\n",
      "forward time 0.389021635055542\n",
      "backward time 0.6683964729309082\n",
      "forward time 0.32459044456481934\n",
      "backward time 0.6036176681518555\n",
      "forward time 0.36011838912963867\n",
      "backward time 0.6070799827575684\n",
      "forward time 0.32185935974121094\n",
      "backward time 0.6145706176757812\n",
      "forward time 0.321044921875\n",
      "backward time 0.6005935668945312\n",
      "forward time 0.37477707862854004\n",
      "backward time 0.5969922542572021\n",
      "forward time 0.3224222660064697\n",
      "backward time 0.5960667133331299\n",
      "forward time 0.32605576515197754\n",
      "backward time 0.6301670074462891\n",
      "forward time 0.38124871253967285\n",
      "backward time 0.6748466491699219\n",
      "forward time 0.3161487579345703\n",
      "backward time 0.7140316963195801\n",
      "forward time 0.3270528316497803\n",
      "backward time 0.5896401405334473\n",
      "forward time 0.3271467685699463\n",
      "backward time 0.6001439094543457\n",
      "forward time 0.32297253608703613\n",
      "backward time 0.6494975090026855\n",
      "forward time 0.3216989040374756\n",
      "backward time 0.6034111976623535\n",
      "forward time 0.33061909675598145\n",
      "backward time 0.78902268409729\n",
      "forward time 0.34570932388305664\n",
      "backward time 0.6847822666168213\n",
      "forward time 0.31533384323120117\n",
      "backward time 0.6822021007537842\n",
      "forward time 0.3825042247772217\n",
      "backward time 0.5988383293151855\n",
      "forward time 0.3269195556640625\n",
      "backward time 0.6754603385925293\n",
      "forward time 0.3216257095336914\n",
      "backward time 0.6815707683563232\n",
      "forward time 0.3389892578125\n",
      "backward time 0.6864006519317627\n",
      "forward time 0.33152318000793457\n",
      "backward time 0.6613550186157227\n",
      "forward time 0.3513660430908203\n",
      "backward time 0.6424829959869385\n",
      "forward time 0.3845338821411133\n",
      "backward time 0.616858720779419\n",
      "forward time 0.3244969844818115\n",
      "backward time 0.7160308361053467\n",
      "forward time 0.48949265480041504\n",
      "backward time 0.6118519306182861\n",
      "forward time 0.42254137992858887\n",
      "backward time 0.6146128177642822\n",
      "forward time 0.4820823669433594\n",
      "backward time 0.7103755474090576\n",
      "forward time 0.42108654975891113\n",
      "backward time 0.6128711700439453\n",
      "forward time 0.3833153247833252\n",
      "backward time 0.6342530250549316\n",
      "forward time 0.33450889587402344\n",
      "backward time 0.7024257183074951\n",
      "forward time 0.37193751335144043\n",
      "backward time 0.760974645614624\n",
      "forward time 0.3823740482330322\n",
      "backward time 0.7495460510253906\n",
      "forward time 0.3582139015197754\n",
      "backward time 0.6598520278930664\n",
      "forward time 0.4081442356109619\n",
      "backward time 0.6995460987091064\n",
      "forward time 0.37300992012023926\n",
      "backward time 0.7033321857452393\n",
      "forward time 0.39960670471191406\n",
      "backward time 0.6542675495147705\n",
      "forward time 0.32216620445251465\n",
      "backward time 0.6409657001495361\n",
      "forward time 0.3288228511810303\n",
      "backward time 0.674487829208374\n",
      "forward time 0.32692575454711914\n",
      "backward time 0.6415295600891113\n",
      "forward time 0.3473498821258545\n",
      "backward time 0.6750609874725342\n",
      "forward time 0.3232901096343994\n",
      "backward time 0.6025924682617188\n",
      "forward time 0.3934636116027832\n",
      "backward time 0.67936110496521\n",
      "forward time 0.3443639278411865\n",
      "backward time 0.6561555862426758\n",
      "forward time 0.34728360176086426\n",
      "backward time 0.6418867111206055\n",
      "forward time 0.3394012451171875\n",
      "backward time 0.6271376609802246\n",
      "forward time 0.32581472396850586\n",
      "backward time 0.7061338424682617\n",
      "forward time 0.34273338317871094\n",
      "backward time 0.7152507305145264\n",
      "forward time 0.32344794273376465\n",
      "backward time 0.7191717624664307\n",
      "forward time 0.3555424213409424\n",
      "backward time 0.6895747184753418\n",
      "forward time 0.34379005432128906\n",
      "backward time 0.6867783069610596\n",
      "forward time 0.3412303924560547\n",
      "backward time 0.660083532333374\n",
      "forward time 0.3457162380218506\n",
      "backward time 0.6967566013336182\n",
      "forward time 0.3422701358795166\n",
      "backward time 0.6033003330230713\n",
      "forward time 0.31661152839660645\n",
      "backward time 0.6362016201019287\n",
      "forward time 0.32203197479248047\n",
      "backward time 0.5970451831817627\n",
      "forward time 0.3149595260620117\n",
      "backward time 0.6884851455688477\n",
      "forward time 0.32153987884521484\n",
      "backward time 0.6842207908630371\n",
      "forward time 0.33727407455444336\n",
      "backward time 0.6699888706207275\n",
      "forward time 0.3223285675048828\n",
      "backward time 0.6722142696380615\n",
      "forward time 0.3383018970489502\n",
      "backward time 0.6876969337463379\n",
      "forward time 0.3502230644226074\n",
      "backward time 0.7022421360015869\n",
      "forward time 0.3300807476043701\n",
      "backward time 0.6398828029632568\n",
      "forward time 0.34290218353271484\n",
      "backward time 0.6841840744018555\n",
      "forward time 0.3581366539001465\n",
      "backward time 0.7120485305786133\n",
      "forward time 0.3328673839569092\n",
      "backward time 0.7089262008666992\n",
      "forward time 0.3780977725982666\n",
      "backward time 0.6033713817596436\n",
      "forward time 0.3783562183380127\n",
      "backward time 0.5991976261138916\n",
      "forward time 0.33667683601379395\n",
      "backward time 0.6126933097839355\n",
      "forward time 0.42044520378112793\n",
      "backward time 0.5971803665161133\n",
      "forward time 0.3335559368133545\n",
      "backward time 0.7071411609649658\n",
      "forward time 0.32137131690979004\n",
      "backward time 0.7351787090301514\n",
      "forward time 0.34424543380737305\n",
      "backward time 0.6916506290435791\n",
      "forward time 0.33893299102783203\n",
      "backward time 0.6296384334564209\n",
      "forward time 0.39496803283691406\n",
      "backward time 0.6665863990783691\n",
      "forward time 0.3922257423400879\n",
      "backward time 0.6402897834777832\n",
      "forward time 0.38164186477661133\n",
      "backward time 0.6244194507598877\n",
      "forward time 0.332813024520874\n",
      "backward time 0.7412099838256836\n",
      "forward time 0.34613919258117676\n",
      "backward time 0.7090790271759033\n",
      "forward time 0.37068963050842285\n",
      "backward time 0.6509664058685303\n",
      "forward time 0.3462684154510498\n",
      "backward time 0.6858797073364258\n",
      "tensor(0.0064, grad_fn=<MeanBackward1>) tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.2524, 0.2175, 0.1715, 0.2625, 0.2271, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], grad_fn=<SelectBackward>) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward time 0.41044068336486816\n",
      "backward time 0.6413588523864746\n",
      "forward time 0.3249797821044922\n",
      "backward time 0.6257538795471191\n",
      "forward time 0.3470931053161621\n",
      "backward time 0.7757046222686768\n",
      "forward time 0.35359930992126465\n",
      "backward time 0.7325537204742432\n",
      "forward time 0.38462185859680176\n",
      "backward time 0.6591188907623291\n",
      "forward time 0.3696286678314209\n",
      "backward time 0.7246279716491699\n",
      "forward time 0.3255951404571533\n",
      "backward time 0.6688945293426514\n",
      "forward time 0.41582560539245605\n",
      "backward time 0.6100254058837891\n",
      "forward time 0.4131484031677246\n",
      "backward time 0.7955288887023926\n",
      "forward time 0.45032715797424316\n",
      "backward time 0.6823406219482422\n",
      "forward time 0.44941282272338867\n",
      "backward time 0.8070919513702393\n",
      "forward time 0.34680604934692383\n",
      "backward time 0.6840474605560303\n",
      "forward time 0.321608304977417\n",
      "backward time 0.6324162483215332\n",
      "forward time 0.3488633632659912\n",
      "backward time 0.7540619373321533\n",
      "forward time 0.33823204040527344\n",
      "backward time 0.7687051296234131\n",
      "forward time 0.3609771728515625\n",
      "backward time 0.6305005550384521\n",
      "forward time 0.40303611755371094\n",
      "backward time 0.6157810688018799\n",
      "forward time 0.332319974899292\n",
      "backward time 0.5999958515167236\n",
      "forward time 0.3315715789794922\n",
      "backward time 0.6735186576843262\n",
      "forward time 0.4611067771911621\n",
      "backward time 0.6014654636383057\n",
      "forward time 0.4083847999572754\n",
      "backward time 0.6122217178344727\n",
      "forward time 0.3359215259552002\n",
      "backward time 0.6015727519989014\n",
      "forward time 0.32073974609375\n",
      "backward time 0.6107032299041748\n",
      "forward time 0.3452723026275635\n",
      "backward time 0.6176497936248779\n",
      "forward time 0.3456423282623291\n",
      "backward time 0.6213517189025879\n",
      "forward time 0.34424471855163574\n",
      "backward time 0.5989758968353271\n",
      "forward time 0.32217955589294434\n",
      "backward time 0.6195464134216309\n",
      "forward time 0.39258384704589844\n",
      "backward time 0.8004069328308105\n",
      "forward time 0.3455808162689209\n",
      "backward time 0.6457524299621582\n",
      "forward time 0.3673875331878662\n",
      "backward time 0.6033473014831543\n",
      "forward time 0.3292505741119385\n",
      "backward time 0.6208012104034424\n",
      "forward time 0.3241233825683594\n",
      "backward time 0.640430212020874\n",
      "forward time 0.38358330726623535\n",
      "backward time 0.7029144763946533\n",
      "forward time 0.36276745796203613\n",
      "backward time 0.7612221240997314\n",
      "forward time 0.35799288749694824\n",
      "backward time 0.6713612079620361\n",
      "forward time 0.391247034072876\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-f637ea398376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mreconstr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes=[1, 100, 100, 100, 2]):\n",
    "        # num_obj_classes includes background class\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            self.add_module('layer_{}'.format(i), self.layers[i])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = F.elu(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "    \n",
    "model = MLP([2, 100, 5])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "model.train()\n",
    "\n",
    "mb_size = 1000\n",
    "latent_size = 100\n",
    "max_shift = 2\n",
    "\n",
    "ind_combs = np.dstack(np.meshgrid(np.arange(100), np.arange(100))).reshape(-1, 2)\n",
    "ind_combs = ind_combs[:, [1,0]]\n",
    "ind_combs = torch.Tensor(ind_combs)\n",
    "# res = []\n",
    "# for i in range(x.shape[0]):\n",
    "#     res.extend(np.concatenate((np.tile(x[i], (x.shape[0], 1)), x), axis=1))\n",
    "# np.array(res)\n",
    "\n",
    "for itr in range(1000):\n",
    "    inds = np.random.randint(2*max_shift, latent_size-2*max_shift, size=mb_size, dtype=np.uint8)\n",
    "    shifts = np.random.randint(-max_shift, max_shift+1, size=mb_size, dtype=np.int8)\n",
    "    \n",
    "    prev = np.zeros((mb_size, latent_size))\n",
    "    cur = np.zeros((mb_size, latent_size))\n",
    "    targ = np.zeros((mb_size, latent_size))\n",
    "    \n",
    "    prev[np.arange(mb_size),inds] = 1\n",
    "    cur[np.arange(mb_size),inds+shifts] = 1\n",
    "    targ[np.arange(mb_size),inds+2*shifts] = 1\n",
    "    \n",
    "    prev = Variable(torch.Tensor(prev), requires_grad=True)\n",
    "    cur = Variable(torch.Tensor(cur), requires_grad=True)\n",
    "    targ = Variable(torch.Tensor(targ), requires_grad=False)\n",
    "    \n",
    "    pred_shifts = model(ind_combs).type(torch.FloatTensor)\n",
    "    reconstr = torch.zeros((mb_size, latent_size+2*max_shift))\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for cur_loc in range(latent_size):\n",
    "        reconstr[:, cur_loc:cur_loc+5] += torch.sum(\n",
    "            cur[:, cur_loc].reshape(mb_size, 1, 1)\n",
    "            * prev[:, :].reshape(mb_size, latent_size, 1)\n",
    "            * pred_shifts[latent_size*cur_loc : latent_size*(cur_loc+1)].repeat(1000, 1, 1),\n",
    "            dim = 1\n",
    "        )\n",
    "    end = time.time()\n",
    "    print('forward time', end-start)\n",
    "    start = time.time()\n",
    "    reconstr = reconstr[:, max_shift:max_shift+latent_size]\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.mean(\n",
    "        (reconstr - targ)**2,\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end = time.time()\n",
    "    print('backward time', end-start)\n",
    "    if itr % 100 == 0:\n",
    "        print(loss, reconstr[0], targ[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:rl]",
   "language": "python",
   "name": "conda-env-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
